---
---


@inproceedings{ma_learning_2024,
	title = {Learning {Autonomous} {Driving} {Tasks} via {Human} {Feedbacks} with {Large} {Language} {Models}},
	abbr = {{EMNLP}},
	author = {Ma, Yunsheng and Cao, Xu and Ye, Wenqian and Cui, Can and Mei, Kai and Wang, Ziran},
    booktitle = {Findings of the Association for Computational Linguistics: EMNLP},
	year = {2024},
    selected = {true},
    preview = {emnlp24.gif},
    bibtex_show={true},
}

@inproceedings{ma_lampilot_2024,
    author = {Yunsheng Ma* and Can Cui* and Xu Cao* and Wenqian Ye and Peiran Liu and Juanwu Lu and Amr Abdelraouf and Rohit Gupta and Kyungtae Han and Aniket Bera and James M. Rehg and Ziran Wang},
    title = {LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year = {2024},
    abbr = {CVPR},
    bibtex_show={true},
    selected={true},
    preview = {lampilot.png},
    arxiv = {2312.04372},
    website = {https://openaccess.thecvf.com/content/CVPR2024/html/Ma_LaMPilot_An_Open_Benchmark_Dataset_for_Autonomous_Driving_with_Language_CVPR_2024_paper.html}
}

@inproceedings{cao_maplm_2024,
    author = {Xu Cao* and Tong Zhou* and Yunsheng Ma* and Wenqian Ye and Can Cui and Kun Tang and Zhipeng Cao and Kaizhao Liang and Ziran Wang and James M. Rehg and Chao Zheng},
    title = {MAPLM: A Real-World Large-Scale Vision-Language Benchmark for Map and Traffic Scene Understanding},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year = {2024},
    abbr = {CVPR},
    bibtex_show={true},
    selected = {true},
    preview = {cover_maplm.png},
    website = {https://openaccess.thecvf.com/content/CVPR2024/html/Cao_MAPLM_A_Real-World_Large-Scale_Vision-Language_Benchmark_for_Map_and_Traffic_CVPR_2024_paper.html}
}

@inproceedings{lu_quantifying_2024,
    author = {Juanwu Lu* and Can Cui* and Yunsheng Ma and Aniket Bera and Ziran Wang},
    title = {Quantifying Uncertainty in Motion Prediction with Variational Bayesian Mixture},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year = {2024},
    abbr = {CVPR},
    selected = {false},
    preview = {cover_seneva.png},
    arxiv = {2404.03789},
}

@article{cui_receive_2024,
    author={Can Cui and Yunsheng Ma and Xu Cao and Wenqian Ye and Ziran Wang},
    journal={IEEE Intelligent Transportation Systems Magazine}, 
    title={Receive, Reason, and React: Drive as You Say, With Large Language Models in Autonomous Vehicles}, 
    year={2024},
    abbr={ITSM},
    doi={10.1109/MITS.2024.3381793},
    selected={false},
    preview={cover_3r.png},
  }


@inproceedings{ma_macp_2024,
    author = {Yunsheng Ma* and Juanwu Lu* and Can Cui and Sicheng Zhao and Xu Cao and Wenqian Ye and Ziran Wang},
    title = {MACP: Efficient Model Adaptation for Cooperative Perception},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
    year = {2024},
    abbr = {WACV},
    selected = {false},
    preview = {cover_macp.png},
    abstract = {Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception capabilities of connected and automated vehicles (CAVs) by enabling information sharing to ``see through the occlusions", resulting in significant performance improvements. However, developing and training complex multi-agent perception models from scratch can be expensive and unnecessary when existing single-agent models show remarkable generalization capabilities. In this paper, we propose a new framework termed MACP, which equips a single-agent pre-trained model with cooperation capabilities. We approach this objective by identifying the key challenges of shifting from single-agent to cooperative settings, adapting the model by freezing most of its parameters and adding a few lightweight modules. We demonstrate in our experiments that the proposed framework can effectively utilize cooperative observations and outperform other state-of-the-art approaches in both simulated and real-world cooperative perception benchmarks while requiring substantially fewer tunable parameters with reduced communication costs.},
    arxiv = {2310.16870},
    code = {https://github.com/PurdueDigitalTwin/MACP},
    website = {https://purduedigitaltwin.github.io/MACP/},
}

@article{ma_driver_2024,
    author = {Yunsheng Ma and Runjia Du and Amr Abdelraouf and Kyungtae Han and Rohit Gupta and Ziran Wang},
    journal = {IEEE Transactions on Intelligent Vehicles},
    title = {Driver Digital Twin for Online Recognition of Distracted Driving Behaviors},
    year = {2024},
    keywords = {Vehicles;Behavioral sciences;Task analysis;Sensors;Location awareness;Cameras;Intelligent vehicles;Digital twin;driver behavior modeling;automated vehicle;driver distraction detection;temporal action localization},
    doi = {10.1109/TIV.2024.3353253},
    abbr = {T-IV},
    selected = {false},
    preview = {cover_ddt.png},
    website = {https://ieeexplore.ieee.org/abstract/document/10398504},
}

@article{cui_redformer_2023,
    author = {Can Cui and Yunsheng Ma and Juanwu Lu and Ziran Wang},
    journal = {IEEE Transactions on Intelligent Vehicles},
    title = {REDFormer: Radar Enlightens the Darkness of Camera Perception With Transformers},
    year = {2023},
    keywords = {Radar;Radar imaging;Object detection;Cameras;Sensor fusion;Three-dimensional displays;Meteorology;Sensor fusion;automated vehicle;computer vision;perception;machine learning},
    doi = {10.1109/TIV.2023.3329708},
    abbr = {T-IV},
    selected = {false},
    preview = {cover_redformer.png},
}


@inproceedings{ye_mitigating_2023,
    title = {Mitigating {Transformer} {Overconfidence} via {Lipschitz} {Regularization}},
    booktitle = {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
    author = {Ye, Wenqian and Ma, Yunsheng and Cao, Xu and Tang, Kun},
    year = {2023},
    selected = {false},
    abbr = {UAI},
    preview = {cover_mitigating.png},
    arxiv = {2306.06849},
}

@inproceedings{ma_cemformer_2023,
    title = {CEMFormer: Learning to Predict Driver Intentions from In-Cabin and External Cameras via Spatial-Temporal Transformers},
    author = {Yunsheng Ma and Wenqian Ye and Xu Cao and Amr Abdelraouf and Kyungtae Han and Rohit Gupta and Ziran Wang},
    booktitle = {IEEE International Conference on Intelligent Transportation Systems},
    arxiv = {2305.07840},
    year = {2023},
    selected = {false},
    abbr = {ITSC},
    preview = {cover_cemformer.png},
}

@inproceedings{ma_m2dar_2023,
    title = {{M2DAR}: {Multi}-{View} {Multi}-{Scale} {Driver} {Action} {Recognition} with {Vision} {Transformer}},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
    author = {Ma, Yunsheng and Yuan, Liangqi and Abdelraouf, Amr and Han, Kyungtae and Gupta, Rohit and Li, Zihao and Wang, Ziran},
    year = {2023},
    selected = {false},
    abbr = {CVPRW},
    preview = {cover_m2dar.png},
    arxiv = {2305.08877},
    award = {Outstanding Speaker Award at NGTS},
    code = {https://github.com/PurdueDigitalTwin/M2DAR},
}

@inproceedings{yuan_peer--peer_2023,
    title = {Peer-to-{Peer} {Federated} {Continual} {Learning} for {Naturalistic} {Driving} {Action} {Recognition}},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
    author = {Yuan, Liangqi and Ma, Yunsheng and Su, Lu and Wang, Ziran},
    year = {2023},
    selected = {false},
    abbr = {CVPRW},
    preview = {cover_p2p.png},
    arxiv = {2304.07421},
}

@inproceedings{zhao_vaanet_2020,
    title = {An {End}-to-{End} {Visual}-{Audio} {Attention} {Network} for {Emotion} {Recognition} in {User}-{Generated} {Videos}},
    booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
    author = {Zhao*, Sicheng and Ma*, Yunsheng and Gu, Yang and Yang, Jufeng and Xing, Tengfei and Xu, Pengfei and Hu, Runbo and Chai, Hua and Keutzer, Kurt},
    year = {2020},
    selected = {false},
    abbr = {AAAI Oral},
    website = {https://ojs.aaai.org/index.php/AAAI/article/view/5364},
    code = {https://github.com/maysonma/VAANet},
    preview = {cover_vaanet.png},
    arxiv = {2003.00832},
}
